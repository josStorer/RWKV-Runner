{
  "Home": "主页",
  "Train": "训练",
  "About": "关于",
  "Settings": "设置",
  "Go to chat page": "前往聊天页",
  "Manage your configs, adjust the starting model and parameters": "管理你的配置, 调整启动的模型和参数",
  "Manage models": "管理模型",
  "Run": "运行",
  "Offline": "离线",
  "Starting": "启动中",
  "Loading": "读取模型中",
  "Working": "运行中",
  "Stop": "停止",
  "Enable High Precision For Last Layer": "输出层使用高精度",
  "Stored Layers": "载入显存层数",
  "Precision": "精度",
  "Device": "设备",
  "Convert model with these configs. Using a converted model will greatly improve the loading speed, but model parameters of the converted model cannot be modified.": "用这些设置转换模型. 使用转换过的模型能大大提升载入速度, 但是转换后的模型无法再修改模型参数.",
  "Manage Models": "管理模型",
  "Model": "模型",
  "Model Parameters": "模型参数",
  "Frequency Penalty": "Frequency Penalty",
  "Presence Penalty": "Presence Penalty",
  "Top_P": "Top_P",
  "Temperature": "Temperature",
  "Max Response Token": "最大响应 Token",
  "API Port": "API 端口",
  "Hover your mouse over the text to view a detailed description. Settings marked with * will take effect immediately after being saved.": "把鼠标悬停在文本上查看详细描述. 标记了星号 * 的设置在保存后会立即生效.",
  "Default API Parameters": "默认 API 参数",
  "Provide JSON file URLs for the models manifest. Separate URLs with semicolons. The \"models\" field in JSON files will be parsed into the following table.": "填写模型描述的 JSON 文件地址. 地址间用分号分隔. JSON 文件内的 \"models\" 字段会被分析进下表.",
  "Config Name": "配置名",
  "Refresh": "刷新",
  "Save Config": "保存配置",
  "Model Source Manifest List": "模型源",
  "Models": "模型",
  "Delete Config": "删除配置",
  "Help": "帮助",
  "Version": "版本",
  "New Config": "新建配置",
  "Open Url": "打开网页",
  "Download": "下载",
  "Open Folder": "打开文件夹",
  "Configs": "配置",
  "Automatic Updates Check": "自动检查更新",
  "Updates Check Error": "检查更新失败",
  "Introduction": "介绍",
  "Dark Mode": "深色模式",
  "Language": "语言",
  "In Development": "开发中",
  "Chat": "聊天",
  "Convert": "转换",
  "Actions": "动作",
  "Last updated": "上次更新",
  "Desc": "描述",
  "Size": "文件大小",
  "File": "文件",
  "Config Saved": "配置已保存",
  "Downloading": "正在下载",
  "Loading Model": "正在读取模型",
  "Startup Completed": "启动完成",
  "Failed to switch model": "切换模型失败",
  "Start Converting": "开始转换",
  "Convert Success": "转换成功",
  "Convert Failed": "转换失败",
  "Model Not Found": "模型不存在",
  "Model Status": "模型状态",
  "Clear": "清除",
  "Send": "发送",
  "Type your message here": "在此输入消息",
  "Copy": "复制",
  "Read Aloud": "朗读",
  "Hello! I'm RWKV, an open-source and commercially usable large language model.": "你好！我是RWKV，一个开源可商用的大语言模型。",
  "This tool's API is compatible with OpenAI API. It can be used with any ChatGPT tool you like. Go to the settings of some ChatGPT tool, replace the 'https://api.openai.com' part in the API address with '": "本工具的API与OpenAI API兼容. 因此可以配合任意你喜欢的ChatGPT工具使用. 打开某个ChatGPT工具的设置, 将API地址中的'https://api.openai.com'部分替换为'",
  "New Version Available": "新版本可用",
  "Update": "更新",
  "Please click the button in the top right corner to start the model": "请点击右上角的按钮启动模型",
  "Update Error": "更新出错",
  "Open the following URL with your browser to view the API documentation": "使用浏览器打开以下地址查看API文档",
  "By default, the maximum number of tokens that can be answered in a single response, it can be changed by the user by specifying API parameters.": "默认情况下, 单个回复最多回答的token数目, 用户可以通过自行指定API参数改变这个值",
  "Sampling temperature, it's like giving alcohol to a model, the higher the stronger the randomness and creativity, while the lower, the more focused and deterministic it will be.": "采样温度, 就像给模型喝酒, 越大随机性越强, 更具创造力, 越小则越保守稳定",
  "Just like feeding sedatives to the model. Consider the results of the top n% probability mass, 0.1 considers the top 10%, with higher quality but more conservative, 1 considers all results, with lower quality but more diverse.": "就像给模型喂镇静剂. 考虑前 n% 概率质量的结果, 0.1 考虑前 10%, 质量更高, 但更保守, 1 考虑所有质量结果, 质量降低, 但更多样",
  "Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.": "存在惩罚. 正值根据新token在至今的文本中是否出现过, 来对其进行惩罚, 从而增加了模型涉及新话题的可能性",
  "Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.": "频率惩罚. 正值根据新token在至今的文本中出现的频率/次数, 来对其进行惩罚, 从而减少模型原封不动地重复相同句子的可能性",
  "int8 uses less VRAM, but has slightly lower quality. fp16 has higher quality.": "int8占用显存更低, 但质量略微下降. fp16质量更好",
  "Number of the neural network layers loaded into VRAM, the more you load, the faster the speed, but it consumes more VRAM. (If your VRAM is not enough, it will fail to load)": "载入显存的神经网络层数, 载入越多, 速度越快, 但显存消耗越大 (如果你的显存不够, 会载入失败)",
  "Whether to use CPU to calculate the last output layer of the neural network with FP32 precision to obtain better quality.": "是否使用cpu以fp32精度计算神经网络的最后一层输出层, 以获得更好的质量",
  "Downloads": "下载",
  "Pause": "暂停",
  "Continue": "继续",
  "Resume": "继续",
  "Check": "查看",
  "Model file not found": "模型文件不存在",
  "Can not find download url": "找不到下载地址",
  "Python target not found, would you like to download it?": "没有找到目标Python, 是否下载?",
  "Python dependencies are incomplete, would you like to install them?": "Python依赖缺失, 是否安装?",
  "Install": "安装",
  "This is the latest version": "已是最新版",
  "Use Alibaba Cloud Pip Mirrors": "使用阿里云Pip镜像源",
  "Model Config Exception": "模型配置异常",
  "Use Gitee Updates Source": "使用Gitee更新源",
  "Use Custom CUDA kernel to Accelerate": "使用自定义CUDA算子加速",
  "Enabling this option can greatly improve inference speed and save some VRAM, but there may be compatibility issues (output garbled). If it fails to start, please turn off this option, or try to upgrade your gpu driver.": "开启这个选项能大大提升推理速度并节省显存，但可能存在兼容性(回复乱码)问题，如果发生相关问题，请关闭此选项。或更新你的显卡驱动",
  "Supported custom cuda file not found": "没有找到支持的自定义cuda文件",
  "Failed to copy custom cuda file": "自定义cuda文件复制失败",
  "Downloading update, please wait. If it is not completed, please manually download the program from GitHub and replace the original program.": "正在下载更新，请等待。如果一直未完成，请从Github手动下载并覆盖原程序",
  "Completion": "续写",
  "Parameters": "参数",
  "Stop Sequences": "停止词",
  "When this content appears in the response result, the generation will end.": "响应结果出现该内容时就结束生成",
  "Reset": "重置",
  "Generate": "生成",
  "Writer": "写作",
  "Translator": "翻译",
  "Catgirl": "猫娘",
  "Code Generation": "代码生成",
  "Werewolf": "狼人杀",
  "Instruction": "指令",
  "Blank": "空白",
  "The following is an epic science fiction masterpiece that is immortalized, with delicate descriptions and grand depictions of interstellar civilization wars.\nChapter 1.\n": "《背影》\n我与父亲不相见已二年余了，我最不能忘记的是他的背影。\n那年冬天，祖母死了，父亲的差使也交卸了，正是祸不单行的日子。我从北京到徐州，打算",
  "The following is a conversation between a cat girl and her owner. The cat girl is a humanized creature that behaves like a cat but is humanoid. At the end of each sentence in the dialogue, she will add \"Meow~\". In the following content, User represents the owner and Assistant represents the cat girl.\n\nUser: Hello.\n\nAssistant: I'm here, meow~.\n\nUser: Can you tell jokes?": "以下是一位猫娘的主人和猫娘的对话内容，猫娘是一种拟人化的生物，其行为似猫但类人，在每一句对话末尾都会加上\"喵~\"。以下内容中，User代表主人，Assistant代表猫娘。\n\nUser: 你好\n\nAssistant: 主人我在哦，喵~\n\nUser: 你会讲笑话吗?",
  "When response finished, inject this content.": "响应结束时，插入此内容到末尾",
  "Inject start text": "起始注入文本",
  "Inject end text": "结尾注入文本",
  "Before the response starts, inject this content.": "响应开始前，在开头插入此内容",
  "There is currently a game of Werewolf with six players, including a Seer (who can check identities at night), two Werewolves (who can choose someone to kill at night), a Bodyguard (who can choose someone to protect at night), two Villagers (with no special abilities), and a game host. User will play as Player 1, Assistant will play as Players 2-6 and the game host, and they will begin playing together. Every night, the host will ask User for his action and simulate the actions of the other players. During the day, the host will oversee the voting process and ask User for his vote. \n\nAssistant: Next, I will act as the game host and assign everyone their roles, including randomly assigning yours. Then, I will simulate the actions of Players 2-6 and let you know what happens each day. Based on your assigned role, you can tell me your actions and I will let you know the corresponding results each day.\n\nUser: Okay, I understand. Let's begin. Please assign me a role. Am I the Seer, Werewolf, Villager, or Bodyguard?\n\nAssistant: You are the Seer. Now that night has fallen, please choose a player to check his identity.\n\nUser: Tonight, I want to check Player 2 and find out his role.": "现在有一场六人狼人杀游戏，包括一名预言家(可以在夜晚查验身份)，两名狼人(可以在夜晚选择杀人)，一名守卫(可以在夜晚选择要守护的人)，两名平民（无技能)，一名主持人，以下内容中User将扮演其中的1号玩家，Assistant来扮演2-6号玩家，以及主持人，并开始与User进行游戏，主持人每晚都会询问User的行动，并模拟其他人的行动，在白天则要主持投票，并同样询问User投票对象，公布投票结果。\n\nAssistant: 接下来，我将首先作为主持人进行角色分配，并给你赋予随机的角色，之后我将模拟2-6号玩家进行行动，告知你每天的动态，根据你被分配的角色，你可以回复我你做的行动，我会告诉你每天对应的结果\n\nUser: 好的，我明白了，那么开始吧。请先给我一个角色身份。我是预言家，狼人，平民，守卫中的哪一个呢？\n\nAssistant: 你的身份是预言家。现在夜晚降临，请选择你要查验的玩家。\n\nUser: 今晚我要验2号玩家，他是什么身份？",
  "Writer, Translator, Role-playing": "写作，翻译，角色扮演",
  "Chinese Kongfu": "情境冒险",
  "Allow external access to the API (service must be restarted)": "允许外部访问API (必须重启服务)",
  "Custom": "自定义",
  "CUDA (Beta, Faster)": "CUDA (Beta, 更快)",
  "Reset All Configs": "重置所有配置",
  "Cancel": "取消",
  "Confirm": "确认",
  "Are you sure you want to reset all configs? This will obtain the latest preset configs, but will override your custom configs and cannot be undone.": "你确定要重置所有配置吗？这会获取最新的预设配置，但会覆盖你的自定义配置，并且无法撤销",
  "Advanced": "高级",
  "Custom Python Path": "自定义Python路径",
  "Custom Models Path": "自定义模型路径",
  "Microsoft Visual C++ Redistributable is not installed, would you like to download it?": "微软VC++组件未安装, 是否下载?",
  "File Path Cannot Contain Space": "文件路径不能包含空格",
  "Current Strategy": "当前Strategy",
  "MacOS is not yet supported for performing this operation, please do it manually.": "MacOS尚未支持此操作, 请手动执行",
  "Linux is not yet supported for performing this operation, please do it manually.": "Linux尚未支持此操作, 请手动执行",
  "On Linux system, you must manually install python dependencies.": "在Linux系统下, 你必须手动安装python依赖",
  "Update completed, please restart the program.": "更新完成, 请重启程序",
  "Are you sure you want to reset this page? It cannot be undone.": "你确定要重置本页吗？这无法撤销",
  "Model file download is not complete": "模型文件下载未完成",
  "Error": "错误",
  "Are you sure you want to clear the conversation? It cannot be undone.": "你确定要清空对话吗？这无法撤销",
  "Save": "保存",
  "Conversation Saved": "对话已保存",
  "Open": "打开",
  "DPI Scaling": "显示缩放",
  "Restart the app to apply DPI Scaling.": "重启应用以使显示缩放生效",
  "Restart": "重启",
  "API Chat Model Name": "API聊天模型名",
  "API Completion Model Name": "API续写模型名",
  "Localhost": "本地",
  "Retry": "重试",
  "Delete": "删除",
  "Edit": "编辑",
  "Memory is not enough, try to increase the virtual memory or use a smaller model.": "内存不足，尝试增加虚拟内存，或使用一个更小规模的模型",
  "Bad PyTorch version, please reinstall PyTorch with cuda.": "错误的PyTorch版本，请重新安装CUDA版本的PyTorch",
  "The model file is corrupted, please download again.": "模型文件损坏，请重新下载",
  "Found no NVIDIA driver, please install the latest driver. If you are not using an Nvidia GPU, please switch the 'Strategy' to WebGPU or CPU in the Configs page.": "没有找到NVIDIA驱动，请安装最新驱动。如果你没有使用Nvidia显卡，请在配置页面将“Strategy”改为WebGPU或CPU",
  "VRAM is not enough, please reduce stored layers or use a lower precision in Configs page.": "显存不足，请在配置页面减少载入显存层数，或使用更低的精度",
  "Failed to enable custom CUDA kernel, ninja is required to load C++ extensions. You may be using the CPU version of PyTorch, please reinstall PyTorch with CUDA. Or if you are using a custom Python interpreter, you must compile the CUDA kernel by yourself or disable Custom CUDA kernel acceleration.": "自定义CUDA算子开启失败，需要安装Ninja来读取C++扩展。你可能正在使用CPU版本的PyTorch，请重新安装CUDA版本的PyTorch。如果你正在使用自定义Python解释器，你必须自己编译CUDA算子或禁用自定义CUDA算子加速",
  "Presets": "预设",
  "Online": "在线",
  "english": "英文",
  "chinese": "中文",
  "default": "默认",
  "japanese": "日文",
  "English": "英文",
  "Chinese": "中文",
  "Default": "默认",
  "Japanese": "日文",
  "New Preset": "新建预设",
  "Import": "导入",
  "Name": "名称",
  "Imported successfully": "导入成功",
  "Failed to import. Please copy a preset to the clipboard.": "导入失败。请复制一个预设到剪贴板",
  "Clipboard is empty.": "剪贴板没有内容",
  "Successfully copied to clipboard.": "成功复制到剪贴板",
  "Edit Character Settings": "编辑人设",
  "Go Back": "返回",
  "Description": "描述",
  "Assistant Avatar Url": "AI头像图片地址",
  "User Avatar Url": "用户头像图片地址",
  "Welcome Message": "欢迎语",
  "Display Preset Messages": "显示预设中的对话",
  "Tag": "标签",
  "Activate": "激活",
  "New": "新建",
  "user": "用户",
  "assistant": "AI",
  "system": "系统",
  "Regenerate": "重新生成",
  "LoRA Finetune": "LoRA微调",
  "Command Stopped": "命令已终止",
  "Please convert data first.": "请先转换数据",
  "Ubuntu is not installed, do you want to install it?": "Ubuntu未安装，是否安装？",
  "Install Ubuntu": "安装Ubuntu",
  "Please install Ubuntu using Microsoft Store, after installation click the Open button in Microsoft Store and then click the Train button": "请用Microsoft Store安装Ubuntu，安装完成后，点击Microsoft Store界面的“打开”按钮，然后点击“训练”按钮",
  "WSL is not enabled, do you want to enable it?": "WSL未启用，是否启用？",
  "Enable WSL": "启用WSL",
  "After installation, please restart your computer to enable WSL": "安装完成后，请重启电脑以启用WSL",
  "Data Process": "数据处理",
  "Data Path": "数据路径",
  "Vocab Path": "词表路径",
  "Train Parameters": "训练参数",
  "Base Model": "基底模型",
  "LoRA Model": "LoRA模型",
  "Merge Model": "合并模型",
  "Devices": "显卡数量",
  "Gradient Checkpoint": "梯度检查点标志",
  "Context Length": "上下文长度",
  "Epoch Steps": "每轮训练步数",
  "Epoch Count": "训练轮次",
  "Epoch Begin": "起始轮次",
  "Epoch Save": "保存间隔轮次",
  "Learning Rate Init": "初始学习率",
  "Learning Rate Final": "最终学习率",
  "Micro Batch Size": "微批次大小",
  "Accumulate Gradient Batches": "梯度累积批次",
  "Warmup Steps": "学习率预热步数",
  "Pre-FFN": "前馈网络预处理",
  "None": "空",
  "Merge model successfully": "合并模型成功",
  "Convert Data successfully": "数据转换成功",
  "Please select a LoRA model": "请选择一个LoRA模型",
  "You are using sample data for training. For formal training, please make sure to create your own jsonl file.": "你正在使用示例数据训练，对于正式训练场合，请务必创建你自己的jsonl训练数据",
  "WSL is not running, please retry. If it keeps happening, it means you may be using an outdated version of WSL, run \"wsl --update\" to update.": "WSL没有运行，请重试。如果一直出现此错误，意味着你可能正在使用旧版本的WSL，请在cmd执行\"wsl --update\"以更新",
  "Memory is not enough, try to increase the virtual memory (Swap of WSL) or use a smaller base model.": "内存不足，尝试增加虚拟内存(WSL Swap)，或使用一个更小规模的基底模型",
  "VRAM is not enough": "显存不足",
  "Training data is not enough, reduce context length or add more data for training": "训练数据不足，请减小上下文长度或增加训练数据",
  "Can not find an Nvidia GPU. Perhaps the gpu driver of windows is too old, or you are using WSL 1 for training, please upgrade to WSL 2. e.g. Run \"wsl --set-version Ubuntu-22.04 2\"": "没有找到Nvidia显卡。可能是因为你的windows显卡驱动太旧，或者你正在使用WSL 1进行训练，请升级到WSL 2。例如，执行\"wsl --set-version Ubuntu-22.04 2\"",
  "Matched CUDA is not installed": "未安装匹配的CUDA",
  "Failed to convert data": "数据转换失败",
  "Failed to merge model": "合并模型失败",
  "The data path should be a directory or a file in jsonl format (more formats will be supported in the future).\n\nWhen you provide a directory path, all the txt files within that directory will be automatically converted into training data. This is commonly used for large-scale training in writing, code generation, or knowledge bases.\n\nThe jsonl format file can be referenced at https://github.com/josStorer/RWKV-Runner/blob/master/finetune/data/sample.jsonl.\nYou can also write it similar to OpenAI's playground format, as shown in https://platform.openai.com/playground/p/default-chat.\nEven for multi-turn conversations, they must be written in a single line using `\\n` to indicate line breaks. If they are different dialogues or topics, they should be written in separate lines.": "数据路径必须是一个文件夹，或者jsonl格式文件 (未来会支持更多格式)\n\n当你填写的路径是一个文件夹时，该文件夹内的所有txt文件会被自动转换为训练数据，通常这用于大批量训练写作，代码生成或知识库\n\njsonl文件的格式参考 https://github.com/josStorer/RWKV-Runner/blob/master/finetune/data/sample.jsonl 以及 https://zhuanlan.zhihu.com/p/643433851\n你也可以仿照openai的playground编写，参考 https://platform.openai.com/playground/p/default-chat\n即使是多轮对话也必须写在一行，用`\\n`表示换行，如果是不同对话或主题，则另起一行",
  "Size mismatch for blocks. You are attempting to continue training from the LoRA model, but it does not match the base model. Please set LoRA model to None.": "尺寸不匹配块。你正在尝试从LoRA模型继续训练，但该LoRA模型与基底模型不匹配，请将LoRA模型设为空",
  "Instruction: Write a story using the following information\n\nInput: A man named Alex chops a tree down\n\nResponse:": "Instruction: Write a story using the following information\n\nInput: 艾利克斯砍倒了一棵树\n\nResponse:",
  "Composition": "作曲",
  "Use Local Sound Font": "使用本地音色资源",
  "Auto Play At The End": "结束时自动播放",
  "No File to save": "无文件可保存",
  "File Saved": "文件已保存",
  "Failed to load local sound font, please check if the files exist - assets/sound-font": "加载本地音色资源失败，请检查文件是否存在 - assets/sound-font",
  "Please convert model to safe tensors format first": "请先将模型转换为Safetensors格式",
  "Convert To Safe Tensors Format": "转换为Safetensors格式",
  "Please change Strategy to WebGPU to use safetensors format": "请将Strategy改为WebGPU以使用safetensors格式",
  "Preview Only": "仅预览",
  "RAM": "内存",
  "VRAM": "显存",
  "GPU Usage": "GPU占用",
  "Use Custom Tokenizer": "使用自定义Tokenizer",
  "Tokenizer Path (e.g. backend-python/rwkv_pip/20B_tokenizer.json or rwkv_vocab_v20230424.txt)": "Tokenizer路径 (例如: backend-python/rwkv_pip/20B_tokenizer.json 或 rwkv_vocab_v20230424.txt)",
  "User Name": "用户名称",
  "Assistant Name": "AI名称",
  "Insert default system prompt at the beginning": "在开头自动插入默认系统提示",
  "Format Content": "规范格式",
  "Add An Attachment (Accepts pdf, txt)": "添加一个附件 (支持pdf, txt)",
  "Processing Attachment": "正在处理附件",
  "Remove Attachment": "移除附件",
  "The content of file": "文件",
  "is as follows. When replying to me, consider the file content and respond accordingly:": "内容如下。回复时考虑文件内容并做出相应回复:",
  "What's the file name": "文件名是什么",
  "The file name is: ": "文件名是：",
  "Port is occupied. Change it in Configs page or close the program that occupies the port.": "端口被占用。请在配置页面更改端口，或关闭占用端口的程序",
  "Loading...": "加载中...",
  "Hello, what can I do for you?": "你好，有什么要我帮忙的吗？",
  "Enable WebUI": "启用WebUI",
  "Server is working on deployment mode, please close the terminal window manually": "服务器正在部署模式下运行，请手动关闭终端窗口",
  "Server is working on deployment mode, please exit the program manually to stop the server": "服务器正在部署模式下运行，请手动退出程序以停止服务器",
  "You can increase the number of stored layers in Configs page to improve performance": "你可以在配置页面增加载入显存层数以提升性能",
  "Failed to load model, try to increase the virtual memory (Swap of WSL) or use a smaller base model.": "模型载入失败，尝试增加虚拟内存(WSL Swap)，或使用一个更小规模的基底模型",
  "Save Conversation": "保存对话",
  "Use Hugging Face Mirror": "使用Hugging Face镜像源",
  "File is empty": "文件为空",
  "Open MIDI Input Audio Tracks": "打开MIDI输入音轨",
  "Track": "音轨",
  "Play All": "播放全部",
  "Clear All": "清空",
  "Scale View": "缩放视图",
  "Record": "录制",
  "Play": "播放",
  "New Track": "新建音轨",
  "Select a track to preview the content": "选择一个音轨以预览内容",
  "Save to generation area": "保存到生成区",
  "Piano": "钢琴",
  "Percussion": "打击乐",
  "Drum": "鼓",
  "Tuba": "大号",
  "Marimba": "马林巴",
  "Bass": "贝斯",
  "Guitar": "吉他",
  "Violin": "小提琴",
  "Trumpet": "小号",
  "Sax": "萨克斯",
  "Flute": "长笛",
  "Lead": "主音",
  "Pad": "和音",
  "MIDI Input": "MIDI输入",
  "Select the MIDI input device to be used.": "选择要使用的MIDI输入设备",
  "Start Time": "开始时间",
  "Content Duration": "内容时长",
  "Please select a MIDI device first": "请先选择一个MIDI设备",
  "Piano is the main instrument": "钢琴为主",
  "Loss is too high, please check the training data, and ensure your gpu driver is up to date.": "Loss过高，请检查训练数据，并确保你的显卡驱动是最新的",
  "This version of RWKV is not supported yet.": "暂不支持此版本的RWKV",
  "Main": "主干",
  "Official": "官方",
  "Finetuned": "微调",
  "Global": "全球",
  "Local": "本地",
  "CN": "中文",
  "JP": "日文",
  "Music": "音乐",
  "Other": "其他",
  "Role Play": "角色扮演",
  "Recommended": "推荐",
  "Import MIDI": "导入MIDI",
  "Current Instrument": "当前乐器",
  "Please convert model to GGML format first": "请先将模型转换为GGML格式",
  "Convert To GGML Format": "转换为GGML格式",
  "CPU (rwkv.cpp, Faster)": "CPU (rwkv.cpp, 更快)",
  "Play With External Player": "使用外部播放器播放",
  "Core API URL": "核心 API URL",
  "Override core API URL(/chat/completions and /completions). If you don't know what this is, leave it blank.": "覆盖核心的 API URL (/chat/completions 和 /completions)。如果你不知道这是什么，请留空",
  "Please change Strategy to CPU (rwkv.cpp) to use ggml format": "请将Strategy改为CPU (rwkv.cpp)以使用ggml格式",
  "Only Auto Play Generated Content": "仅自动播放新生成的内容",
  "Model has been converted and does not match current strategy. If you are using a new strategy, re-convert the model.": "所选模型已被转换过，并且不匹配当前的Strategy。如果你正在使用新的Strategy，请重新转换模型",
  "Instruction 1": "指令1",
  "Instruction 2": "指令2",
  "Instruction 3": "指令3",
  "Instruction: You are an expert assistant for summarizing and extracting information from given content\nGenerate a valid JSON in the following format:\n{\n    \"summary\": \"Summary of content\",\n    \"keywords\": [\"content keyword 1\", \"content keyword 2\"]\n}\n\nInput: The open-source community has introduced Eagle 7B, a new RNN model, built on the RWKV-v5 architecture. This new model has been trained on 1.1 trillion tokens and supports over 100 languages. The RWKV architecture, short for ‘Rotary Weighted Key-Value,’ is a type of architecture used in the field of artificial intelligence, particularly in natural language processing (NLP) and is a variation of the Recurrent Neural Network (RNN) architecture.\nEagle 7B promises lower inference cost and stands out as a leading 7B model in terms of environmental efficiency and language versatility.\nThe model, with its 7.52 billion parameters, shows excellent performance in multi-lingual benchmarks, setting a new standard in its category. It competes closely with larger models in English language evaluations and is distinctive as an “Attention-Free Transformer,” though it requires additional tuning for specific uses. This model is accessible under the Apache 2.0 license and can be downloaded from HuggingFace for both personal and commercial purposes.\nIn terms of multilingual performance, Eagle 7B has claimed to have achieved notable results in benchmarks covering 23 languages. Its English performance has also seen significant advancements, outperforming its predecessor, RWKV v4, and competing with top-tier models.\nWorking towards a more scalable architecture and use of data efficiently, Eagle 7B is a more inclusive AI technology, supporting a broader range of languages. This model challenges the prevailing dominance of transformer models by demonstrating the capabilities of RNNs like RWKV in achieving superior performance when trained on comparable data volumes.\nIn the RWKV model, the rotary mechanism transforms the input data in a way that helps the model better understand the position or or order of elements in a sequence. The weighted key value also makes the model efficient by retrieving the stored information from previous elements in a sequence. \nHowever, questions remain about the scalability of RWKV compared to transformers, although there is optimism regarding its potential. The team plans to include additional training, an in-depth paper on Eagle 7B, and the development of a 2T model.\n\nResponse: {": "Instruction: 你是一个专业的内容分析总结助手\n根据提供的内容生成以下格式的有效JSON信息:\n{\n    \"summary\": \"内容的简短摘要\",\n    \"keywords\": [\"内容关键词 1\", \"内容关键词 2\"]\n}\n\nInput: 开源社区推出了基于RWKV-v5架构的Eagle 7B新的RNN模型。这个新模型以1.1万亿个token进行了训练，并支持100多种语言。RWKV架构是人工智能领域中特别是自然语言处理（NLP）中使用的一种架构，它是循环神经网络（RNN）架构的一种变种。\nEagle 7B承诺低推理成本，并以其环境效益和语言灵活性在领先的7B模型中脱颖而出。\n该模型拥有75.2亿个参数，在多语言基准测试中表现出色，树立了新的行业标准。它在英语语言评估中与更大的模型竞争激烈，并作为“无注意力Transformer”独具特色，尽管它需要针对特定用途进行额外调整。该模型可在Apache 2.0许可下访问，并可从HuggingFace下载，用于个人和商业目的。\n关于多语言性能，Eagle 7B声称在涵盖23种语言的基准测试中取得了显著成绩。它的英语性能也取得了重大进步，超越了它的前身RWKV v4，并与顶级模型竞争。\n为了实现更可扩展的架构和有效利用数据，Eagle 7B是一种更包容的人工智能技术，支持更广泛的语言范围。通过展示RWKV等RNNs在训练相当数据量时实现卓越性能的能力，该模型挑战了Transformer模型的主导地位。\n在RWKV模型中，旋转机制以一种有助于模型更好地理解序列中元素的位置或顺序的方式转换输入数据。加权关键值还通过从序列中先前元素中检索存储的信息，使模型更高效。\n然而，与Transformer相比，人们对RWKV的可扩展性仍然存在疑问，尽管对其潜力持乐观态度。团队计划包括额外的训练、对Eagle 7B进行深入论文研究以及开发一个2T模型。\n\nResponse: {",
  "Penalty Decay": "惩罚衰减",
  "If you don't know what it is, keep it default.": "如果你不知道这是什么，保持默认",
  "Failed to find the base model, please try to change your base model.": "未找到基底模型，请尝试更换基底模型",
  "Markdown Renderer": "Markdown渲染",
  "Load Conversation": "读取对话",
  "The latest X messages will be sent to the server. If you are using the RWKV-Runner server, please use the default value because RWKV-Runner has built-in state cache management which only calculates increments. Sending all messages will have lower cost. If you are using ChatGPT, adjust this value according to your needs to reduce ChatGPT expenses.": "最近的X条消息会发送至服务器. 如果你正在使用RWKV-Runner服务器, 请使用默认值, 因为RWKV-Runner内置了state缓存管理, 只计算增量, 发送所有消息将具有更低的成本. 如果你正在使用ChatGPT, 则根据你的需要调整此值, 这可以降低ChatGPT的费用",
  "History Message Number": "历史消息数量",
  "Send All Message": "发送所有消息",
  "Quantized Layers": "量化层数",
  "Number of the neural network layers quantized with current precision, the more you quantize, the lower the VRAM usage, but the quality correspondingly decreases.": "神经网络以当前精度量化的层数, 量化越多, 占用显存越低, 但质量相应下降",
  "Parallel Token Chunk Size": "并行Token块大小",
  "Maximum tokens to be processed in parallel at once. For high end GPUs, this could be 64 or 128 (faster).": "一次最多可以并行处理的token数量. 对于高端显卡, 这可以是64或128 (更快)",
  "Global Penalty": "全局惩罚",
  "When generating a response, whether to include the submitted prompt as a penalty factor. By turning this off, you will get the same generated results as official RWKV Gradio. If you find duplicate results in the generated results, turning this on can help avoid generating duplicates.": "生成响应时, 是否将提交的prompt也纳入到惩罚项. 关闭此项将得到与RWKV官方Gradio完全一致的生成结果. 如果你发现生成结果出现重复, 那么开启此项有助于避免生成重复",
  "Create a new user or AI message content. You can prepare a chat record with AI here, and fill in the responses you want to get from AI in the tone of AI. When you use this preset, the chat record will be processed, and at this point, AI will better understand what you want it to do or what role to play.": "新建一个 用户 或 AI 的发言内容. 你可以在这里准备好一段你与 AI 的聊天记录, 并用 AI 的口吻正确填写你想得到的 AI 的回复, 这样你在使用这个预设时, 这些聊天记录也会被处理, 并且此时 AI 能更好地理解你希望它做什么 / 扮演什么样的角色",
  "The name used internally by the model when processing user message, changing this value helps improve the role-playing effect.": "模型内部处理用户发言时使用的名称, 更改此值有助于改善角色扮演效果",
  "The name used internally by the model when processing AI message, changing this value helps improve the role-playing effect.": "模型内部处理AI发言时使用的名称, 更改此值有助于改善角色扮演效果",
  "Inside the model, there is a default prompt to improve the model's handling of common issues, but it may degrade the role-playing effect. You can disable this option to achieve a better role-playing effect.": "模型内部有一个默认提示来改善模型处理常规问题的效果, 但它可能会让角色扮演的效果变差, 你可以关闭此选项来获得更好的角色扮演效果",
  "Exit without saving": "退出而不保存",
  "Content has been changed, are you sure you want to exit without saving?": "内容已经被修改, 你确定要退出而不保存吗?",
  "Don't forget to correctly fill in your Ollama API Chat Model Name.": "不要忘记正确填写你的Ollama API 聊天模型名",
  "State-tuned Model": "State微调模型",
  "See More": "查看更多",
  "State Model": "State模型",
  "State model mismatch": "State模型不匹配",
  "File format of the model or state model not supported": "模型或state模型的文件格式不支持",
  "Note: You are using an English state": "注意: 你正在使用一个英文state",
  "Note: You are using a Chinese state": "注意: 你正在使用一个中文state",
  "Note: You are using a Japanese state": "注意: 你正在使用一个日文state",
  "What's the weather like in Paris?": "巴黎的天气怎么样?",
  "Function Call": "函数调用",
  "Tool Definition": "工具定义",
  "Tool Return Value": "工具返回值",
  "Tool Definition is not a valid JSON": "工具定义不是一个有效的JSON",
  "Current selected model may not support function call": "当前选择的模型可能不支持函数调用"
}
