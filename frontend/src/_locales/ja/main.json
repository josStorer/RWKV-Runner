{
  "Home": "ホーム",
  "Train": "トレーニング",
  "About": "約",
  "Settings": "設定",
  "Go to chat page": "チャットページに移動する",
  "Manage your configs, adjust the starting model and parameters": "設定を管理し、開始モデルとパラメータを調整します",
  "Manage models": "モデルの管理",
  "Run": "実行",
  "Offline": "オフライン",
  "Starting": "起動中",
  "Loading": "モデルを読み込み中",
  "Working": "動作中",
  "Stop": "停止",
  "Enable High Precision For Last Layer": "最後の層で高精度を有効にする",
  "Stored Layers": "保存されるレイヤー",
  "Precision": "精度",
  "Device": "デバイス",
  "Convert model with these configs. Using a converted model will greatly improve the loading speed, but model parameters of the converted model cannot be modified.": "これらの設定でモデルを変換します。変換されたモデルを使用すると、読み込み速度が大幅に向上しますが、変換したモデルのパラメータを変更することはできません。",
  "Manage Models": "モデルの管理",
  "Model": "モデル",
  "Model Parameters": "モデルのパラメータ",
  "Frequency Penalty": "周波数のペナルティ",
  "Presence Penalty": "存在のペナルティ",
  "Top_P": "Top_P",
  "Temperature": "温度",
  "Max Response Token": "最大レスポンストークン",
  "API Port": "API ポート",
  "Hover your mouse over the text to view a detailed description. Settings marked with * will take effect immediately after being saved.": "マウスをテキストに一定時間置いて詳細な説明を表示します。 * が付いている設定は保存後すぐに有効化されます。",
  "Default API Parameters": "デフォルトのAPIパラメータ",
  "Provide JSON file URLs for the models manifest. Separate URLs with semicolons. The \"models\" field in JSON files will be parsed into the following table.": "モデルマニフェストのためのJSONファイルURLを提供します。URLはセミコロンで分割します。JSONファイルの\"models\"フィールドは次の表に解析されます。",
  "Config Name": "構成名",
  "Refresh": "リフレッシュ",
  "Save Config": "構成を保存",
  "Model Source Manifest List": "モデルソースマニフェストリスト",
  "Models": "モデル",
  "Delete Config": "設定を削除",
  "Help": "ヘルプ",
  "Version": "バージョン",
  "New Config": "新たな設定",
  "Open Url": "URLを開く",
  "Download": "ダウンロード",
  "Open Folder": "フォルダを開く",
  "Configs": "設定",
  "Automatic Updates Check": "自動更新チェック",
  "Updates Check Error": "更新チェックエラー",
  "Introduction": "序文",
  "Dark Mode": "ダークモード",
  "Language": "言語",
  "In Development": "開発中",
  "Chat": "チャット",
  "Convert": "変更",
  "Actions": "行動",
  "Last updated": "最後に更新",
  "Desc": "説明",
  "Size": "サイズ",
  "File": "ファイル",
  "Config Saved": "設定が保存されました",
  "Downloading": "ダウンロード中",
  "Loading Model": "モデルを読み込んでいます",
  "Startup Completed": "起動完了",
  "Failed to switch model": "モデルの切り替えに失敗しました",
  "Start Converting": "変換を開始",
  "Convert Success": "変換成功",
  "Convert Failed": "変換失敗",
  "Model Not Found": "モデルが見つかりません",
  "Model Status": "モデルの状態",
  "Clear": "クリア",
  "Send": "送信",
  "Type your message here": "ここにメッセージを入力してください",
  "Copy": "コピー",
  "Read Aloud": "読み上げ",
  "Hello! I'm RWKV, an open-source and commercially usable large language model.": "こんにちは！私はRWKV、オープンソースで商用利用可能な大規模な言語モデルです。",
  "This tool's API is compatible with OpenAI API. It can be used with any ChatGPT tool you like. Go to the settings of some ChatGPT tool, replace the 'https://api.openai.com' part in the API address with '": "このツールのAPIはOpenAI APIと互換性があります。 お好きなChatGPTツールで使用することができます。いくつかのChatGPTツールの設定に移動し、APIアドレスの 'https://api.openai.com' 部分を '",
  "New Version Available": "新しいバージョンが存在します",
  "Update": "更新",
  "Please click the button in the top right corner to start the model": "右上角のボタンをクリックしてモデルを起動してください",
  "Update Error": "更新エラー",
  "Open the following URL with your browser to view the API documentation": "以下のURLをブラウザで開いてAPIドキュメンテーションを確認してください",
  "By default, the maximum number of tokens that can be answered in a single response, it can be changed by the user by specifying API parameters.": "デフォルトでは、一度に回答できるトークンの最大数は、APIパラメータを指定することでユーザーが変更できます。",
  "Sampling temperature, it's like giving alcohol to a model, the higher the stronger the randomness and creativity, while the lower, the more focused and deterministic it will be.": "サンプリング温度は、モデルにアルコールを与えるようなもので、高いほどランダム性と創造性が強く、低いほど焦点を絞り、決定論的になります。",
  "Just like feeding sedatives to the model. Consider the results of the top n% probability mass, 0.1 considers the top 10%, with higher quality but more conservative, 1 considers all results, with lower quality but more diverse.": "モデルに鎮静剤を与えるようなもの。上位n％の確率質量の結果を考えてみてください。0.1は上位10％を考えており、質が高いが保守的で、1は全ての結果を考慮しており、質は低いが多様性があります。",
  "Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.": "ポジティヴ値は、新しいトークンが今までのテキストに出現していたかどうかに基づいてこれらをペナルティとし、新しいトピックについて話す可能性を増加させます。",
  "Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.": "ポジティブ値は、新しいトークンが既存のテキストでどれだけ頻繁に使われているかに基づいてペナルティを与え、モデルが同じ行を完全に繰り返す可能性を減らします。",
  "int8 uses less VRAM, but has slightly lower quality. fp16 has higher quality.": "int8はVRAMの使用量が少ないですが、質が若干低いです。fp16は高品質。",
  "Number of the neural network layers loaded into VRAM, the more you load, the faster the speed, but it consumes more VRAM. (If your VRAM is not enough, it will fail to load)": "VRAMにロードされるニューラルネットワークの層の数。ロードする量が多いほど速度は速くなりますが、VRAMを多く消費します。(VRAMが不足している場合、ロードに失敗します)",
  "Whether to use CPU to calculate the last output layer of the neural network with FP32 precision to obtain better quality.": "ネットワークの最終出力層をFP32精度で計算するためにCPUを使用するかどうか。",
  "Downloads": "ダウンロード",
  "Pause": "ポーズ",
  "Continue": "続行",
  "Resume": "続行",
  "Check": "確認",
  "Model file not found": "モデルファイルが見つかりません",
  "Can not find download url": "ダウンロードURLが見つかりません",
  "Python target not found, would you like to download it?": "Pythonターゲットが見つかりません、ダウンロードしますか？",
  "Python dependencies are incomplete, would you like to install them?": "Pythonの依存関係が不完全です、インストールしますか？",
  "Install": "インストール",
  "This is the latest version": "これは最新バージョンです",
  "Use Alibaba Cloud Pip Mirrors": "Alibaba Cloud Pipミラーサーバーを使用",
  "Model Config Exception": "モデル設定例外",
  "Use Gitee Updates Source": "Gitee更新ソースを使用",
  "Use Custom CUDA kernel to Accelerate": "カスタムCUDAカーネルを使用して加速",
  "Enabling this option can greatly improve inference speed and save some VRAM, but there may be compatibility issues (output garbled). If it fails to start, please turn off this option, or try to upgrade your gpu driver.": "このオプションを有効にすると、推論速度が大幅に向上し、一部のVRAMを節約できますが、互換性の問題 (文字化けを出力する) が生じる可能性があります。起動に失敗した場合は、このオプションを無効にするか、GPUドライバーをアップグレードしてみてください。",
  "Supported custom cuda file not found": "対応しているカスタムCUDAファイルが見つかりません",
  "Failed to copy custom cuda file": "カスタムCUDAファイルのコピーに失敗しました",
  "Downloading update, please wait. If it is not completed, please manually download the program from GitHub and replace the original program.": "更新をダウンロード中です、お待ちください。完了しない場合は、GitHubから手動でプログラムをダウンロードし、元のプログラムを置き換えてください。",
  "Completion": "補完",
  "Parameters": "パラメータ",
  "Stop Sequences": "シーケンスを停止",
  "When this content appears in the response result, the generation will end.": "この内容が応答結果に表示されると、生成が終了します。",
  "Reset": "リセット",
  "Generate": "生成",
  "Writer": "ライター",
  "Translator": "翻訳者",
  "Catgirl": "ネコガール",
  "Code Generation": "コード生成",
  "Werewolf": "人狼",
  "Instruction": "指示",
  "Blank": "空白",
  "The following is an epic science fiction masterpiece that is immortalized, with delicate descriptions and grand depictions of interstellar civilization wars.\nChapter 1.\n": "以下は、壮大な描写と共に、不滅のエピックサイエンスフィクションの傑作で、星間文明戦争が繊細に描かれています。\n第1章\n",
  "The following is a conversation between a cat girl and her owner. The cat girl is a humanized creature that behaves like a cat but is humanoid. At the end of each sentence in the dialogue, she will add \"Meow~\". In the following content, User represents the owner and Assistant represents the cat girl.\n\nUser: Hello.\n\nAssistant: I'm here, meow~.\n\nUser: Can you tell jokes?": "以下は、猫少女とその飼い主との会話です。猫少女は、猫のように振る舞いながらもヒトの姿をした生物です。会話の各文の終わりには必ず「にゃ〜」とつけています。以下の文章では、Userが飼い主、Assistantが猫少女を表しています。\n\nUser: こんにちは。\n\nAssistant: ここにいますよ、にゃ〜。\n\nUser: 笑い話を話せますか？",
  "When response finished, inject this content.": "応答終了時に、この内容を注入します。",
  "Inject start text": "開始テキストを注入",
  "Inject end text": "終了テキストを注入",
  "Before the response starts, inject this content.": "応答が始まる前に、この内容を注入します。",
  "There is currently a game of Werewolf with six players, including a Seer (who can check identities at night), two Werewolves (who can choose someone to kill at night), a Bodyguard (who can choose someone to protect at night), two Villagers (with no special abilities), and a game host. User will play as Player 1, Assistant will play as Players 2-6 and the game host, and they will begin playing together. Every night, the host will ask User for his action and simulate the actions of the other players. During the day, the host will oversee the voting process and ask User for his vote. \n\nAssistant: Next, I will act as the game host and assign everyone their roles, including randomly assigning yours. Then, I will simulate the actions of Players 2-6 and let you know what happens each day. Based on your assigned role, you can tell me your actions and I will let you know the corresponding results each day.\n\nUser: Okay, I understand. Let's begin. Please assign me a role. Am I the Seer, Werewolf, Villager, or Bodyguard?\n\nAssistant: You are the Seer. Now that night has fallen, please choose a player to check his identity.\n\nUser: Tonight, I want to check Player 2 and find out his role.": "現在、6人のプレイヤーが参加する人狼ゲームが行われています。その中には、夜に任意のプレイヤーの正体を確認できる占い師、夜に誰かを殺すことができる人狼2名、夜に誰かを守ることができるボディガード、特殊な能力を持っていない村人2名、そしてゲームのホストがいます。Userはプレイヤー1として、Assistantはプレーヤー2から6まで及びゲームのホストとして参加し、一緒にゲームを始めます。ホストは毎晩、Userに彼の行動を問い、他のプレーヤーの行動をシミュレートします。昼には、ホストが投票プロセスを監督し、Userに彼の投票を求めます。\n\nAssistant: 次に、私はゲームのホストとして参加者全員に役割を割り当てることになります。それには、あなたの役割もランダムに割り当てます。その後、私はプレーヤー2から6の行動をシミュレートし、毎日何が起こったかを報告します。あなたに割り当てられた役割に基づいて、あなたの行動を教えてください。私は毎日、それに対する結果を報告します。\n\nUser: 了解しました。では、始めましょう。私の役割を割り当ててください。占い師、人狼、村人、ボディーガードのいずれなのでしょうか？\n\nAssistant: あなたの役割は占い師です。今夜が来たので、誰の正体を確認するか選んでください。\n\nUser: 今夜、プレイヤー2の役割を確認したい。",
  "Writer, Translator, Role-playing": "ライター、翻訳者、ロールプレイング",
  "Chinese Kongfu": "中国武術",
  "Allow external access to the API (service must be restarted)": "APIへの外部アクセスを許可する (サービスを再起動する必要があります)",
  "Custom": "カスタム",
  "CUDA (Beta, Faster)": "CUDA (Beta, 高速)",
  "Reset All Configs": "すべての設定をリセット",
  "Cancel": "キャンセル",
  "Confirm": "確認",
  "Are you sure you want to reset all configs? This will obtain the latest preset configs, but will override your custom configs and cannot be undone.": "本当にすべての設定をリセットしますか？これにより最新のプリセット設定が取得されますが、カスタム設定は上書きされ、元に戻すことはできません。",
  "Advanced": "高度な",
  "Custom Python Path": "カスタムPythonパス",
  "Custom Models Path": "カスタムモデルパス",
  "Microsoft Visual C++ Redistributable is not installed, would you like to download it?": "Microsoft Visual C++ 再頒布可能パッケージがインストールされていません。ダウンロードしますか？",
  "File Path Cannot Contain Space": "ファイルのパスにスペースを含めることはできません",
  "Current Strategy": "現在の戦略",
  "MacOS is not yet supported for performing this operation, please do it manually.": "MacOSはまだこの操作を実行するサポートがありませんので、手動で行ってください。",
  "Linux is not yet supported for performing this operation, please do it manually.": "Linuxはまだこの操作を実行するサポートがありませんので、手動で行ってください。",
  "On Linux system, you must manually install python dependencies.": "Linuxシステムでは、pythonの依存関係を手動でインストールする必要があります。",
  "Update completed, please restart the program.": "更新が完了したら、プログラムを再起動してください。",
  "Are you sure you want to reset this page? It cannot be undone.": "本当にこのページをリセットしてもよろしいですか？元に戻すことはできません。",
  "Model file download is not complete": "モデルファイルのダウンロードが完了していません",
  "Error": "エラー",
  "Are you sure you want to clear the conversation? It cannot be undone.": "会話をクリアしてもよろしいですか？元に戻すことはできません。",
  "Save": "保存",
  "Conversation Saved": "会話が保存されました",
  "Open": "開く",
  "DPI Scaling": "DPIスケーリング",
  "Restart the app to apply DPI Scaling.": "DPIスケーリングを適用するためにアプリを再起動してください。",
  "Restart": "再起動",
  "API Chat Model Name": "APIチャットモデル名",
  "API Completion Model Name": "API完成モデル名",
  "Localhost": "ローカルホスト",
  "Retry": "リトライ",
  "Delete": "削除",
  "Edit": "編集",
  "Memory is not enough, try to increase the virtual memory or use a smaller model.": "メモリが不足しています。仮想メモリを増やすか、もしくは小さなモデルを使ってみてください",
  "Bad PyTorch version, please reinstall PyTorch with cuda.": "不適切なPyTorchのバージョンです。cudaと共にPyTorchを再インストールしてください。",
  "The model file is corrupted, please download again.": "モデルファイルが破損しています。再度ダウンロードしてください。",
  "Found no NVIDIA driver, please install the latest driver. If you are not using an Nvidia GPU, please switch the 'Strategy' to WebGPU or CPU in the Configs page.": "NVIDIAのドライバが見つかりません。最新版のドライバをインストールしてください。NvidiaのGPUを使用していない場合は、設定ページで\"Strategy\"をWebGPUまたはCPUに切り替えてください。",
  "VRAM is not enough, please reduce stored layers or use a lower precision in Configs page.": "VRAMが足りません。設定ページで保存されているレイヤーを減らすか、精度を下げてください。",
  "Failed to enable custom CUDA kernel, ninja is required to load C++ extensions. You may be using the CPU version of PyTorch, please reinstall PyTorch with CUDA. Or if you are using a custom Python interpreter, you must compile the CUDA kernel by yourself or disable Custom CUDA kernel acceleration.": "カスタムCUDAカーネルの有効化に失敗しました。C++拡張を読み込むためにはNinjaが必要です。あなたは恐らくCPU版のPyTorchを使用しており、CUDA版のPyTorchを再インストールする必要があります。または、あなたがカスタムPythonインタプリタを使用している場合は、CUDAカーネルを自分でコンパイルするか、カスタムCUDAカーネルのアクセラレーションを無効にする必要があります。",
  "Presets": "プリセット",
  "Online": "オンライン",
  "english": "英語",
  "chinese": "中国語",
  "default": "デフォルト",
  "japanese": "日本語",
  "English": "英語",
  "Chinese": "中国語",
  "Default": "デフォルト",
  "Japanese": "日本語",
  "New Preset": "新規プリセット",
  "Import": "インポート",
  "Name": "名前",
  "Imported successfully": "インポート成功",
  "Failed to import. Please copy a preset to the clipboard.": "インポートに失敗しました。プリセットをクリップボードにコピーしてください。",
  "Clipboard is empty.": "クリップボードが空です。",
  "Successfully copied to clipboard.": "クリップボードにコピーしました。",
  "Edit Character Settings": "キャラクター設定を編集",
  "Go Back": "戻る",
  "Description": "説明",
  "Assistant Avatar Url": "アシスタントアバターURL",
  "User Avatar Url": "ユーザーアバターURL",
  "Welcome Message": "ウェルカムメッセージ",
  "Display Preset Messages": "プリセットメッセージの表示",
  "Tag": "タグ",
  "Activate": "アクティブ化",
  "New": "新規",
  "user": "ユーザー",
  "assistant": "アシスタント",
  "system": "システム",
  "Regenerate": "再生成",
  "LoRA Finetune": "LoRAの微調整",
  "Command Stopped": "コマンドが停止しました",
  "Please convert data first.": "先にデータを変換してください。",
  "Ubuntu is not installed, do you want to install it?": "Ubuntuがインストールされていません、インストールしますか？",
  "Install Ubuntu": "Ubuntuをインストール",
  "Please install Ubuntu using Microsoft Store, after installation click the Open button in Microsoft Store and then click the Train button": "UbuntuをMicrosoftストアからインストールすることができます。インストールが完了したら、MicrosoftストアのOpenボタンを押し、Trainボタンを押してください",
  "WSL is not enabled, do you want to enable it?": "WSLが有効になっていません、有効化しますか？",
  "Enable WSL": "WSLを有効化",
  "After installation, please restart your computer to enable WSL": "インストールが完了したら、WSLを有効化するためにコンピュータを再起動してください",
  "Data Process": "データ処理",
  "Data Path": "データパス",
  "Vocab Path": "語彙パス",
  "Train Parameters": "トレーニングパラメータ",
  "Base Model": "基本モデル",
  "LoRA Model": "LoRAモデル",
  "Merge Model": "モデルの統合",
  "Devices": "デバイス",
  "Gradient Checkpoint": "勾配チェックポイント",
  "Context Length": "コンテキストの長さ",
  "Epoch Steps": "エポックステップ数",
  "Epoch Count": "エポックの数",
  "Epoch Begin": "エポックの起点",
  "Epoch Save": "エポックの保存",
  "Learning Rate Init": "初期学習率",
  "Learning Rate Final": "最終学習率",
  "Micro Batch Size": "マイクロバッチサイズ",
  "Accumulate Gradient Batches": "勾配バッチの累計",
  "Warmup Steps": "ウォームアップステップ",
  "Pre-FFN": "FFNの前処理",
  "None": "なし",
  "Merge model successfully": "モデルのマージが成功しました",
  "Convert Data successfully": "データ変換に成功しました",
  "Please select a LoRA model": "LoRAモデルを選択してください",
  "You are using sample data for training. For formal training, please make sure to create your own jsonl file.": "トレーニングにはサンプルデータを使用しています。正式なトレーニングのためには、自身でjsonlファイルを作成してください。",
  "WSL is not running, please retry. If it keeps happening, it means you may be using an outdated version of WSL, run \"wsl --update\" to update.": "WSLが実行されていません、もう一度試してください。これが続く場合、古いバージョンのWSLを使用している可能性があります。\"wsl --update\"を実行して更新してください。",
  "Memory is not enough, try to increase the virtual memory (Swap of WSL) or use a smaller base model.": "メモリが不足しています、仮想メモリ (WSL Swap) を増やすか小さなベースモデルを使用してみてください。",
  "VRAM is not enough": "ビデオRAMが不足しています",
  "Training data is not enough, reduce context length or add more data for training": "トレーニングデータが不足しています、コンテキストの長さを減らすか、トレーニング用のデータをさらに追加してください",
  "Can not find an Nvidia GPU. Perhaps the gpu driver of windows is too old, or you are using WSL 1 for training, please upgrade to WSL 2. e.g. Run \"wsl --set-version Ubuntu-22.04 2\"": "Nvidia GPUが見つかりません。WindowsのGPUドライバが古すぎるか、トレーニングにWSL 1を使用している可能性があります。WSL 2にアップグレードしてください。例：\"wsl --set-version Ubuntu-22.04 2\"を実行してください",
  "Matched CUDA is not installed": "対応するCUDAがインストールされていません",
  "Failed to convert data": "データの変換に失敗しました",
  "Failed to merge model": "モデルのマージに失敗しました",
  "The data path should be a directory or a file in jsonl format (more formats will be supported in the future).\n\nWhen you provide a directory path, all the txt files within that directory will be automatically converted into training data. This is commonly used for large-scale training in writing, code generation, or knowledge bases.\n\nThe jsonl format file can be referenced at https://github.com/josStorer/RWKV-Runner/blob/master/finetune/data/sample.jsonl.\nYou can also write it similar to OpenAI's playground format, as shown in https://platform.openai.com/playground/p/default-chat.\nEven for multi-turn conversations, they must be written in a single line using `\\n` to indicate line breaks. If they are different dialogues or topics, they should be written in separate lines.": "データのパスはディレクトリまたはjsonl形式のファイルでなければなりません（将来的にはより多くの形式がサポートされる予定です）。ディレクトリパスを提供した場合、そのディレクトリ内のすべてのtxtファイルが自動的にトレーニングデータに変換されます。これは大規模なライティング、コード生成、または知識ベースのトレーニングで一般的に使用されます。jsonl形式のファイルは、https://github.com/josStorer/RWKV-Runner/blob/master/finetune/data/sample.jsonl を参照してください。\nhttps://platform.openai.com/playground/p/default-chat のように、OpenAIのプレイグラウンド形式に似た形式で書くこともできます。複数ターンの対話であっても、一行で書く必要があり、行の区切りを示すために`\\n`を使用します。それらが異なる対話やトピックであれば、それらは別々の行に書かれるべきです。",
  "Size mismatch for blocks. You are attempting to continue training from the LoRA model, but it does not match the base model. Please set LoRA model to None.": "ブロックのサイズが一致しません。LoRAモデルからトレーニングを続けようとしていますが、それはベースモデルと一致しません。LoRAモデルをNoneに設定してください。",
  "Instruction: Write a story using the following information\n\nInput: A man named Alex chops a tree down\n\nResponse:": "Instruction: Write a story using the following information\n\nInput: アレックスという男が木を切り倒す\n\nResponse:",
  "Composition": "作曲",
  "Use Local Sound Font": "ローカルサウンドフォントを使用する",
  "Auto Play At The End": "最後に自動再生",
  "No File to save": "保存するファイルがありません",
  "File Saved": "ファイルが保存されました",
  "Failed to load local sound font, please check if the files exist - assets/sound-font": "ローカルサウンドフォントの読み込みに失敗しました、ファイルが存在するか確認してください - assets/sound-font",
  "Please convert model to safe tensors format first": "モデルを安全なテンソル形式に変換してください",
  "Convert To Safe Tensors Format": "安全なテンソル形式に変換",
  "Please change Strategy to WebGPU to use safetensors format": "StrategyをWebGPUに変更して、安全なテンソル形式を使用してください",
  "Preview Only": "プレビューのみ",
  "RAM": "RAM",
  "VRAM": "VRAM",
  "GPU Usage": "GPU使用率",
  "Use Custom Tokenizer": "カスタムトークナイザーを使用する",
  "Tokenizer Path (e.g. backend-python/rwkv_pip/20B_tokenizer.json or rwkv_vocab_v20230424.txt)": "トークナイザーパス (例: backend-python/rwkv_pip/20B_tokenizer.json または rwkv_vocab_v20230424.txt)",
  "User Name": "ユーザー名",
  "Assistant Name": "アシスタント名",
  "Insert default system prompt at the beginning": "最初にデフォルトのシステムプロンプトを挿入",
  "Format Content": "内容フォーマットの規格化",
  "Add An Attachment (Accepts pdf, txt)": "添付ファイルを追加 (pdf, txtを受け付けます)",
  "Processing Attachment": "添付ファイルを処理中",
  "Remove Attachment": "添付ファイルを削除",
  "The content of file": "ファイル",
  "is as follows. When replying to me, consider the file content and respond accordingly:": "の内容は以下の通りです。私に返信する際は、ファイルの内容を考慮して適切に返信してください:",
  "What's the file name": "ファイル名は何ですか",
  "The file name is: ": "ファイル名は次のとおりです: ",
  "Port is occupied. Change it in Configs page or close the program that occupies the port.": "ポートが占有されています。設定ページで変更するか、ポートを占有しているプログラムを終了してください。",
  "Loading...": "読み込み中...",
  "Hello, what can I do for you?": "こんにちは、何かお手伝いできますか？",
  "Enable WebUI": "WebUIを有効化",
  "Server is working on deployment mode, please close the terminal window manually": "サーバーはデプロイモードで動作しています、ターミナルウィンドウを手動で閉じてください",
  "Server is working on deployment mode, please exit the program manually to stop the server": "サーバーはデプロイモードで動作しています、サーバーを停止するにはプログラムを手動で終了してください",
  "You can increase the number of stored layers in Configs page to improve performance": "パフォーマンスを向上させるために、保存されるレイヤーの数を設定ページで増やすことができます",
  "Failed to load model, try to increase the virtual memory (Swap of WSL) or use a smaller base model.": "モデルの読み込みに失敗しました、仮想メモリ (WSL Swap) を増やすか小さなベースモデルを使用してみてください。",
  "Save Conversation": "会話を保存",
  "Use Hugging Face Mirror": "Hugging Faceミラーを使用",
  "File is empty": "ファイルが空です",
  "Open MIDI Input Audio Tracks": "MIDI入力オーディオトラックを開く",
  "Track": "トラック",
  "Play All": "すべて再生",
  "Clear All": "すべてクリア",
  "Scale View": "スケールビュー",
  "Record": "録音",
  "Play": "再生",
  "New Track": "新規トラック",
  "Select a track to preview the content": "トラックを選択して内容をプレビュー",
  "Save to generation area": "生成エリアに保存",
  "Piano": "ピアノ",
  "Percussion": "パーカッション",
  "Drum": "ドラム",
  "Tuba": "チューバ",
  "Marimba": "マリンバ",
  "Bass": "ベース",
  "Guitar": "ギター",
  "Violin": "バイオリン",
  "Trumpet": "トランペット",
  "Sax": "サックス",
  "Flute": "フルート",
  "Lead": "リード",
  "Pad": "パッド",
  "MIDI Input": "MIDI入力",
  "Select the MIDI input device to be used.": "使用するMIDI入力デバイスを選択します。",
  "Start Time": "開始時間",
  "Content Duration": "内容の長さ",
  "Please select a MIDI device first": "まずMIDIデバイスを選択してください",
  "Piano is the main instrument": "ピアノはメインの楽器です",
  "Loss is too high, please check the training data, and ensure your gpu driver is up to date.": "Lossが大きすぎます、トレーニングデータを確認し、GPUドライバが最新であることを確認してください。",
  "This version of RWKV is not supported yet.": "このバージョンのRWKVはまだサポートされていません。",
  "Main": "メイン",
  "Official": "公式",
  "Finetuned": "微調整",
  "Global": "グローバル",
  "Local": "ローカル",
  "CN": "中国語",
  "JP": "日本語",
  "Music": "音楽",
  "Other": "その他",
  "Role Play": "ロールプレイ",
  "Recommended": "おすすめ",
  "Import MIDI": "MIDIをインポート",
  "Current Instrument": "現在の楽器",
  "Please convert model to GGML format first": "モデルをGGML形式に変換してください",
  "Convert To GGML Format": "GGML形式に変換",
  "CPU (rwkv.cpp, Faster)": "CPU (rwkv.cpp, 高速)",
  "Play With External Player": "外部プレーヤーで再生",
  "Core API URL": "コアAPI URL",
  "Override core API URL(/chat/completions and /completions). If you don't know what this is, leave it blank.": "コアAPI URLを上書きします(/chat/completions と /completions)。何であるかわからない場合は空白のままにしてください。",
  "Please change Strategy to CPU (rwkv.cpp) to use ggml format": "StrategyをCPU (rwkv.cpp)に変更して、ggml形式を使用してください",
  "Only Auto Play Generated Content": "生成されたコンテンツのみ自動再生",
  "Model has been converted and does not match current strategy. If you are using a new strategy, re-convert the model.": "モデルが変換され、現在の戦略と一致しません。新しい戦略を使用している場合は、モデルを再変換してください。",
  "Instruction 1": "指示 1",
  "Instruction 2": "指示 2",
  "Instruction 3": "指示 3",
  "Instruction: You are an expert assistant for summarizing and extracting information from given content\nGenerate a valid JSON in the following format:\n{\n    \"summary\": \"Summary of content\",\n    \"keywords\": [\"content keyword 1\", \"content keyword 2\"]\n}\n\nInput: The open-source community has introduced Eagle 7B, a new RNN model, built on the RWKV-v5 architecture. This new model has been trained on 1.1 trillion tokens and supports over 100 languages. The RWKV architecture, short for ‘Rotary Weighted Key-Value,’ is a type of architecture used in the field of artificial intelligence, particularly in natural language processing (NLP) and is a variation of the Recurrent Neural Network (RNN) architecture.\nEagle 7B promises lower inference cost and stands out as a leading 7B model in terms of environmental efficiency and language versatility.\nThe model, with its 7.52 billion parameters, shows excellent performance in multi-lingual benchmarks, setting a new standard in its category. It competes closely with larger models in English language evaluations and is distinctive as an “Attention-Free Transformer,” though it requires additional tuning for specific uses. This model is accessible under the Apache 2.0 license and can be downloaded from HuggingFace for both personal and commercial purposes.\nIn terms of multilingual performance, Eagle 7B has claimed to have achieved notable results in benchmarks covering 23 languages. Its English performance has also seen significant advancements, outperforming its predecessor, RWKV v4, and competing with top-tier models.\nWorking towards a more scalable architecture and use of data efficiently, Eagle 7B is a more inclusive AI technology, supporting a broader range of languages. This model challenges the prevailing dominance of transformer models by demonstrating the capabilities of RNNs like RWKV in achieving superior performance when trained on comparable data volumes.\nIn the RWKV model, the rotary mechanism transforms the input data in a way that helps the model better understand the position or or order of elements in a sequence. The weighted key value also makes the model efficient by retrieving the stored information from previous elements in a sequence. \nHowever, questions remain about the scalability of RWKV compared to transformers, although there is optimism regarding its potential. The team plans to include additional training, an in-depth paper on Eagle 7B, and the development of a 2T model.\n\nResponse: {": "Instruction: You are an expert assistant for summarizing and extracting information from given content\nGenerate a valid JSON in the following format:\n{\n    \"summary\": \"Summary of content\",\n    \"keywords\": [\"content keyword 1\", \"content keyword 2\"]\n}\n\nInput: The open-source community has introduced Eagle 7B, a new RNN model, built on the RWKV-v5 architecture. This new model has been trained on 1.1 trillion tokens and supports over 100 languages. The RWKV architecture, short for ‘Rotary Weighted Key-Value,’ is a type of architecture used in the field of artificial intelligence, particularly in natural language processing (NLP) and is a variation of the Recurrent Neural Network (RNN) architecture.\nEagle 7B promises lower inference cost and stands out as a leading 7B model in terms of environmental efficiency and language versatility.\nThe model, with its 7.52 billion parameters, shows excellent performance in multi-lingual benchmarks, setting a new standard in its category. It competes closely with larger models in English language evaluations and is distinctive as an “Attention-Free Transformer,” though it requires additional tuning for specific uses. This model is accessible under the Apache 2.0 license and can be downloaded from HuggingFace for both personal and commercial purposes.\nIn terms of multilingual performance, Eagle 7B has claimed to have achieved notable results in benchmarks covering 23 languages. Its English performance has also seen significant advancements, outperforming its predecessor, RWKV v4, and competing with top-tier models.\nWorking towards a more scalable architecture and use of data efficiently, Eagle 7B is a more inclusive AI technology, supporting a broader range of languages. This model challenges the prevailing dominance of transformer models by demonstrating the capabilities of RNNs like RWKV in achieving superior performance when trained on comparable data volumes.\nIn the RWKV model, the rotary mechanism transforms the input data in a way that helps the model better understand the position or or order of elements in a sequence. The weighted key value also makes the model efficient by retrieving the stored information from previous elements in a sequence. \nHowever, questions remain about the scalability of RWKV compared to transformers, although there is optimism regarding its potential. The team plans to include additional training, an in-depth paper on Eagle 7B, and the development of a 2T model.\n\nResponse: {",
  "Penalty Decay": "ペナルティ減衰",
  "If you don't know what it is, keep it default.": "何であるかわからない場合はデフォルトのままにしてください。",
  "Failed to find the base model, please try to change your base model.": "ベースモデルが見つかりませんでした、ベースモデルを変更してみてください。",
  "Markdown Renderer": "Markdownレンダリング",
  "Load Conversation": "会話を読み込む",
  "The latest X messages will be sent to the server. If you are using the RWKV-Runner server, please use the default value because RWKV-Runner has built-in state cache management which only calculates increments. Sending all messages will have lower cost. If you are using ChatGPT, adjust this value according to your needs to reduce ChatGPT expenses.": "最新のX件のメッセージがサーバーに送信されます。RWKV-Runnerサーバーを使用している場合は、デフォルト値を使用してください。RWKV-Runnerには組み込みの状態キャッシュ管理があり、増分のみを計算します。すべてのメッセージを送信すると、コストが低くなります。ChatGPTを使用している場合は、ChatGPTの費用を削減するために必要に応じてこの値を調整してください。",
  "History Message Number": "履歴メッセージ数",
  "Send All Message": "すべてのメッセージを送信",
  "Quantized Layers": "量子化されたレイヤー",
  "Number of the neural network layers quantized with current precision, the more you quantize, the lower the VRAM usage, but the quality correspondingly decreases.": "現在の精度で量子化されたニューラルネットワークのレイヤーの数、量子化するほどVRAMの使用量が低くなりますが、品質も相応に低下します。",
  "Parallel Token Chunk Size": "並列トークンチャンクサイズ",
  "Maximum tokens to be processed in parallel at once. For high end GPUs, this could be 64 or 128 (faster).": "一度に並列で処理される最大トークン数。高性能なGPUの場合、64または128になります（高速）。",
  "Global Penalty": "グローバルペナルティ",
  "When generating a response, whether to include the submitted prompt as a penalty factor. By turning this off, you will get the same generated results as official RWKV Gradio. If you find duplicate results in the generated results, turning this on can help avoid generating duplicates.": "レスポンスを生成する際、提出されたプロンプトをペナルティ要因として含めるかどうか。これをオフにすると、公式RWKV Gradioと同じ生成結果を得ることができます。生成された結果に重複がある場合、これをオンにすることで重複の生成を回避するのに役立ちます。",
  "Create a new user or AI message content. You can prepare a chat record with AI here, and fill in the responses you want to get from AI in the tone of AI. When you use this preset, the chat record will be processed, and at this point, AI will better understand what you want it to do or what role to play.": "新しいユーザーまたはAIメッセージコンテンツを作成します。ここでAIとのチャット記録を準備し、AIから得たい応答をAIのトーンで記入することができます。このプリセットを使用すると、チャット記録が処理され、この時点でAIはあなたが望むことやどのような役割を果たすかをよりよく理解することができます。",
  "The name used internally by the model when processing user message, changing this value helps improve the role-playing effect.": "ユーザーメッセージを処理する際にモデルが内部で使用する名前、この値を変更することで、役割演技の効果を向上させることができます。",
  "The name used internally by the model when processing AI message, changing this value helps improve the role-playing effect.": "AIメッセージを処理する際にモデルが内部で使用する名前、この値を変更することで、役割演技の効果を向上させることができます。",
  "Inside the model, there is a default prompt to improve the model's handling of common issues, but it may degrade the role-playing effect. You can disable this option to achieve a better role-playing effect.": "モデル内部には、一般的な問題の処理を改善するためのデフォルトのプロンプトがありますが、役割演技の効果を低下させる可能性があります。このオプションを無効にすることで、より良い役割演技効果を得ることができます。",
  "Exit without saving": "保存せずに終了",
  "Content has been changed, are you sure you want to exit without saving?": "コンテンツが変更されています、保存せずに終了してもよろしいですか？",
  "Don't forget to correctly fill in your Ollama API Chat Model Name.": "Ollama APIチャットモデル名を正しく記入するのを忘れないでください。",
  "State-tuned Model": "State調整モデル",
  "See More": "もっと見る",
  "State Model": "Stateモデル",
  "State model mismatch": "Stateモデルの不一致",
  "File format of the model or state model not supported": "モデルまたはStateモデルのファイル形式がサポートされていません",
  "Note: You are using an English state": "注意: あなたは英語のstateを使用しています",
  "Note: You are using a Chinese state": "注意: あなたは中国語のstateを使用しています",
  "Note: You are using a Japanese state": "注意: あなたは日本語のstateを使用しています",
  "What's the weather like in Paris?": "パリの天気はどうですか？",
  "Function Call": "関数呼び出し",
  "Tool Definition": "ツール定義",
  "Tool Return Value": "ツールの戻り値",
  "Tool Definition is not a valid JSON": "ツール定義が有効なJSONではありません",
  "Current selected model may not support function call": "現在選択されているモデルは、関数呼び出しをサポートしていない可能性があります"
}
